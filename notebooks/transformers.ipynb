{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertModel, BertTokenizer\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = pd.DataFrame(\n",
    "    [x.rstrip() for x\n",
    "     in open(\"../data/rt-polaritydata/rt-polarity.pos\").readlines()],\n",
    "columns=['review'])\n",
    "pos['type'] = 'positive'\n",
    "neg = pd.DataFrame(\n",
    "    [x.rstrip() for x\n",
    "     in open(\"../data/rt-polaritydata/rt-polarity.neg\").readlines()],\n",
    "columns=['review'])\n",
    "neg['type'] = 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PROP = 0.8\n",
    "train_pos = pos.sample(frac=TRAIN_PROP)\n",
    "train_neg = neg.sample(frac=TRAIN_PROP)\n",
    "train_reviews = pd.concat([train_pos, train_neg])\n",
    "test_pos = pos.drop(train_pos.index)\n",
    "test_neg = neg.drop(train_neg.index)\n",
    "test_reviews = pd.concat([test_pos, test_neg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "config = BertConfig.from_pretrained(\"bert-base-uncased\", output_hidden_states=True)\n",
    "model = BertModel.from_pretrained('bert-base-uncased', config=config)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use BERT to create sentence representations (13 layers x n words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors = []\n",
    "test_vectors = []\n",
    "with torch.no_grad():\n",
    "    for x in train_reviews['review']:\n",
    "        input_ids = torch.tensor([tokenizer.encode(x, add_special_tokens=True)])\n",
    "        output = model(input_ids)\n",
    "        train_vectors.append((output, input_ids, x))\n",
    "    for x in test_reviews['review']:\n",
    "        input_ids = torch.tensor([tokenizer.encode(x, add_special_tokens=True)])\n",
    "        output = model(input_ids)\n",
    "        test_vectors.append((output, input_ids, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([[[-0.1197, -0.5103,  0.1755,  ..., -0.4528,  0.5111,  0.3569],\n",
       "           [-0.0253, -0.4410,  0.0922,  ..., -0.4358,  0.9313,  0.2552],\n",
       "           [ 0.1569, -0.8916,  0.9990,  ..., -0.0488,  0.7035, -0.1455],\n",
       "           ...,\n",
       "           [ 0.0419,  0.0642,  0.2044,  ..., -0.0439,  0.3478, -0.2916],\n",
       "           [-0.0450, -1.2345, -0.0363,  ...,  0.6918,  0.7434, -0.7393],\n",
       "           [ 0.8274,  0.0998, -0.2487,  ..., -0.0129, -0.5028, -0.2075]]]),\n",
       "  tensor([[-8.3739e-01, -3.9210e-01, -8.1751e-01,  7.1130e-01,  7.4057e-01,\n",
       "           -5.4031e-02,  6.3976e-01,  1.4254e-01, -6.7329e-01, -9.9995e-01,\n",
       "           -5.5910e-01,  9.0345e-01,  9.6192e-01,  2.5095e-01,  8.4693e-01,\n",
       "           -6.0248e-01, -1.7685e-01, -5.2174e-01,  2.3617e-01, -2.0786e-01,\n",
       "            5.9117e-01,  9.9995e-01,  1.1312e-01,  2.6169e-01,  4.2185e-01,\n",
       "            9.8173e-01, -5.6391e-01,  8.8605e-01,  9.0437e-01,  7.4216e-01,\n",
       "           -3.2492e-01,  2.2916e-01, -9.8062e-01, -6.5900e-02, -7.5941e-01,\n",
       "           -9.8279e-01,  3.7656e-01, -6.3605e-01,  2.5971e-01,  1.0343e-01,\n",
       "           -8.3494e-01,  1.8559e-01,  9.9991e-01, -6.4216e-02,  3.4479e-01,\n",
       "           -8.7124e-02, -1.0000e+00,  2.1560e-01, -8.3334e-01,  8.4828e-01,\n",
       "            7.5761e-01,  8.6124e-01,  1.5004e-01,  4.5982e-01,  3.9448e-01,\n",
       "           -2.9477e-01, -1.0897e-01,  1.1652e-01, -2.0477e-01, -5.2128e-01,\n",
       "           -4.8439e-01,  2.2323e-01, -8.2601e-01, -8.2568e-01,  8.6882e-01,\n",
       "            6.2675e-01, -1.7903e-01, -2.5834e-01, -5.6532e-02, -1.3658e-01,\n",
       "            7.3077e-01,  3.6281e-02, -1.0118e-01, -7.5059e-01,  5.1195e-01,\n",
       "            2.4609e-01, -5.3255e-01,  1.0000e+00, -4.5278e-01, -9.6745e-01,\n",
       "            6.3616e-01,  4.9624e-01,  4.8938e-01, -5.5007e-02,  1.5394e-01,\n",
       "           -1.0000e+00,  5.6436e-01, -3.9427e-02, -9.8220e-01,  8.2046e-02,\n",
       "            6.5193e-01, -2.2501e-01,  1.4850e-01,  4.4618e-01, -5.3564e-02,\n",
       "           -3.7725e-01, -2.8029e-01, -8.2646e-01, -1.6418e-01, -4.3346e-01,\n",
       "            5.6860e-02, -1.0430e-01, -1.2041e-02, -1.5836e-01,  3.1974e-01,\n",
       "           -4.7863e-01, -3.0377e-01,  5.6771e-01, -1.1050e-02,  5.8314e-01,\n",
       "            3.5375e-01, -3.0555e-01,  4.3902e-01, -9.3895e-01,  5.9735e-01,\n",
       "           -2.0392e-01, -9.7896e-01, -5.3616e-01, -9.8450e-01,  5.6073e-01,\n",
       "           -2.1341e-01, -1.5711e-01,  9.2177e-01, -1.0207e-01,  2.5335e-01,\n",
       "            8.2581e-02, -8.2757e-01, -1.0000e+00, -6.8393e-01, -4.4487e-01,\n",
       "           -3.2925e-02, -3.7406e-01, -9.6754e-01, -9.5684e-01,  5.1370e-01,\n",
       "            9.4423e-01,  7.6236e-02,  9.9948e-01, -2.2319e-01,  8.8398e-01,\n",
       "           -1.4456e-01, -5.4946e-01,  6.6162e-01, -3.9633e-01,  6.3602e-01,\n",
       "           -1.6875e-01, -4.0811e-01,  1.7149e-01, -5.0252e-01,  1.4073e-01,\n",
       "           -6.5648e-01, -1.8193e-01, -6.0857e-01, -8.9306e-01, -1.9653e-01,\n",
       "            8.9382e-01, -5.6252e-01, -7.5822e-01,  1.7135e-01, -2.0205e-01,\n",
       "           -3.8569e-01,  7.2503e-01,  6.5035e-01,  2.9894e-01, -3.0947e-01,\n",
       "            3.2975e-01, -1.0383e-01,  4.8604e-01, -6.9059e-01, -8.0209e-02,\n",
       "            3.1147e-01, -3.3976e-01, -7.2264e-01, -9.6765e-01, -3.1468e-01,\n",
       "            4.0514e-01,  9.7944e-01,  6.4355e-01,  2.2983e-01,  6.5271e-01,\n",
       "           -2.4033e-01,  5.8127e-01, -9.0567e-01,  9.6722e-01, -1.1953e-01,\n",
       "            1.7120e-01, -4.2124e-01,  4.1024e-01, -8.3170e-01, -4.1748e-01,\n",
       "            6.6403e-01, -7.1118e-01, -7.6991e-01, -6.9465e-03, -3.2825e-01,\n",
       "           -3.0382e-01, -7.3465e-01,  4.5910e-01, -1.5997e-01, -2.2302e-01,\n",
       "           -6.2512e-02,  8.6390e-01,  9.0232e-01,  6.6998e-01, -2.6330e-02,\n",
       "            6.7227e-01, -7.9702e-01, -2.9649e-01,  5.7574e-02,  9.7902e-02,\n",
       "            3.7461e-02,  9.8726e-01, -2.7954e-01, -7.5722e-02, -8.7258e-01,\n",
       "           -9.7325e-01, -1.1935e-01, -8.8378e-01, -1.2207e-01, -5.8631e-01,\n",
       "            4.2158e-01, -5.4110e-01,  2.6640e-01,  1.6091e-01, -9.3393e-01,\n",
       "           -6.1581e-01,  2.7772e-01, -1.8852e-01,  4.6185e-01, -1.0111e-01,\n",
       "            7.9538e-01,  9.1805e-01, -5.2169e-01,  5.4520e-01,  8.7702e-01,\n",
       "           -7.5792e-01, -8.0325e-01,  4.4229e-01, -1.9705e-01,  7.3784e-01,\n",
       "           -4.7390e-01,  9.8107e-01,  8.0621e-01,  6.4300e-01, -8.9785e-01,\n",
       "           -3.1302e-01, -7.7417e-01, -3.2654e-01, -3.1082e-02, -3.0726e-01,\n",
       "            7.5934e-01,  4.6952e-01,  4.0006e-01,  6.3570e-01, -4.4754e-01,\n",
       "            9.6801e-01, -8.8415e-01, -9.3940e-01, -6.9721e-01,  1.5572e-01,\n",
       "           -9.8279e-01,  8.0347e-01,  2.6101e-01,  3.4181e-01, -3.6076e-01,\n",
       "           -5.0836e-01, -9.3491e-01,  7.8681e-01,  9.3919e-02,  9.6280e-01,\n",
       "           -5.4366e-01, -8.0368e-01, -7.2679e-01, -8.7536e-01, -1.3538e-01,\n",
       "           -1.6911e-01, -3.5029e-01, -1.2201e-01, -9.2089e-01,  5.0119e-01,\n",
       "            4.7407e-01,  3.3891e-01, -7.9875e-01,  9.9306e-01,  1.0000e+00,\n",
       "            9.3707e-01,  8.1379e-01,  6.2614e-01, -9.9948e-01, -8.5835e-01,\n",
       "            9.9998e-01, -9.8598e-01, -1.0000e+00, -8.7793e-01, -5.6775e-01,\n",
       "            3.3290e-02, -1.0000e+00, -6.2375e-02,  5.5917e-02, -8.7836e-01,\n",
       "            4.1931e-01,  9.6339e-01,  9.5605e-01, -1.0000e+00,  8.2858e-01,\n",
       "            9.1485e-01, -5.1061e-01,  9.3018e-01, -2.9618e-01,  9.4957e-01,\n",
       "            4.4054e-01,  5.3988e-01, -7.7018e-02,  3.3418e-01, -8.4843e-01,\n",
       "           -7.2933e-01, -4.7443e-01, -5.6739e-01,  9.9680e-01,  1.1176e-01,\n",
       "           -7.5725e-01, -8.0224e-01,  5.5717e-01, -1.1620e-01, -1.4662e-01,\n",
       "           -9.2325e-01, -1.1251e-01,  4.3948e-01,  7.1419e-01,  1.0665e-01,\n",
       "            1.2554e-01, -5.4977e-01,  1.9763e-01, -1.6699e-01, -6.1439e-02,\n",
       "            5.5967e-01, -9.0914e-01, -4.3712e-01, -6.9251e-01, -1.5559e-02,\n",
       "           -1.0732e-01, -9.1144e-01,  9.1717e-01, -3.6065e-01,  7.9800e-01,\n",
       "            1.0000e+00,  4.0934e-01, -7.4560e-01,  5.7052e-01,  2.1295e-01,\n",
       "           -6.6543e-01,  1.0000e+00,  7.7621e-01, -9.6738e-01, -4.7178e-01,\n",
       "            6.1912e-01, -4.8100e-01, -5.7250e-01,  9.9801e-01, -2.2022e-01,\n",
       "           -5.7618e-01, -2.1811e-01,  9.6772e-01, -9.8676e-01,  9.8911e-01,\n",
       "           -7.8273e-01, -9.5179e-01,  9.3555e-01,  8.7835e-01, -4.5398e-01,\n",
       "           -6.6052e-01,  9.3382e-02, -2.1468e-01,  2.7363e-01, -9.3127e-01,\n",
       "            7.3820e-01,  2.4839e-01,  7.8994e-03,  8.5510e-01, -7.2133e-01,\n",
       "           -4.6068e-01,  2.9166e-01, -7.6974e-01, -7.4820e-02,  8.4754e-01,\n",
       "            4.7786e-01, -1.0280e-01, -3.3955e-04, -1.6306e-01, -5.9830e-01,\n",
       "           -9.4088e-01,  4.7621e-01,  1.0000e+00, -1.5142e-01,  6.8477e-01,\n",
       "           -1.5978e-01,  4.1325e-04, -1.7302e-01,  4.8356e-01,  4.9759e-01,\n",
       "           -1.7478e-01, -7.0363e-01,  7.7780e-01, -8.5381e-01, -9.8019e-01,\n",
       "            6.1139e-01,  1.3174e-01, -5.0978e-02,  9.9992e-01,  3.1471e-01,\n",
       "            2.5325e-01,  3.6586e-01,  9.7790e-01, -9.7027e-02,  1.5785e-01,\n",
       "            7.3443e-01,  9.6930e-01, -7.6648e-02,  5.5870e-01,  7.5932e-01,\n",
       "           -7.6257e-01, -4.3952e-02, -5.8171e-01, -6.1247e-03, -8.8181e-01,\n",
       "            1.6880e-01, -9.4740e-01,  9.5233e-01,  9.2092e-01,  3.6251e-01,\n",
       "            1.0830e-01,  6.4510e-01,  1.0000e+00, -7.3614e-01,  3.9999e-01,\n",
       "            2.6203e-02,  4.5428e-01, -9.9945e-01, -5.5148e-01, -3.4983e-01,\n",
       "            6.3686e-02, -7.1450e-01, -2.5677e-01,  2.2137e-01, -9.5448e-01,\n",
       "            5.8546e-01,  6.8091e-01, -9.1244e-01, -9.7726e-01, -2.8549e-01,\n",
       "            4.7623e-01, -2.7238e-02, -9.8118e-01, -5.5575e-01, -4.7469e-01,\n",
       "            5.4795e-01, -1.3170e-01, -9.1359e-01,  3.7855e-02, -1.7650e-01,\n",
       "            5.0155e-01, -1.1294e-01,  5.2383e-01,  5.7576e-01,  8.8281e-01,\n",
       "           -7.8566e-01, -1.1110e-01, -1.7462e-01, -6.8640e-01,  8.2934e-01,\n",
       "           -7.5758e-01, -8.5219e-01, -2.9111e-02,  1.0000e+00, -4.9053e-01,\n",
       "            7.0256e-01,  5.3909e-01,  6.1590e-01, -1.4247e-01,  1.1457e-01,\n",
       "            8.1526e-01,  2.1463e-01, -4.3300e-01, -8.5152e-01, -1.1755e-01,\n",
       "           -2.2991e-01,  4.6018e-01,  5.0450e-01,  2.7178e-01,  7.1503e-01,\n",
       "            7.5893e-01,  1.9884e-01,  7.3404e-02, -9.5231e-02,  9.9641e-01,\n",
       "            1.3962e-02, -4.8429e-02, -3.4705e-01, -7.3812e-02, -2.5244e-01,\n",
       "            4.9010e-02,  1.0000e+00,  2.9362e-01,  3.1305e-01, -9.8024e-01,\n",
       "           -7.3217e-01, -8.5706e-01,  1.0000e+00,  7.9067e-01, -7.1718e-01,\n",
       "            5.8046e-01,  4.6680e-01, -1.1243e-01,  6.0910e-01, -1.4611e-01,\n",
       "           -1.3352e-01,  1.0422e-01,  3.0091e-02,  9.2036e-01, -5.2803e-01,\n",
       "           -9.5813e-01, -3.4596e-01,  4.1631e-01, -9.2877e-01,  9.9961e-01,\n",
       "           -5.3565e-01, -9.9507e-02, -2.7073e-01, -4.3980e-01, -6.6557e-02,\n",
       "            1.4090e-02, -9.6050e-01, -1.7849e-01,  2.2371e-01,  9.4349e-01,\n",
       "            1.3342e-01, -5.2708e-01, -8.2659e-01,  8.3199e-01,  7.0420e-01,\n",
       "           -8.1416e-01, -9.1342e-01,  9.3151e-01, -9.5781e-01,  5.9539e-01,\n",
       "            1.0000e+00,  3.1607e-01,  2.9593e-01,  8.3278e-02, -1.7077e-01,\n",
       "            3.3858e-01, -4.0823e-01,  5.7350e-01, -9.2226e-01, -2.7643e-01,\n",
       "           -1.2472e-01,  3.7150e-01, -2.3582e-01, -2.2771e-01,  5.5823e-01,\n",
       "            1.9412e-02, -4.0437e-01, -6.1356e-01,  3.6600e-02,  3.6686e-01,\n",
       "            7.1650e-01, -2.3111e-01, -1.0604e-02,  1.5956e-01,  3.1853e-02,\n",
       "           -8.3800e-01, -2.9020e-01, -2.1662e-01, -9.9998e-01,  6.1396e-01,\n",
       "           -1.0000e+00,  5.5066e-01, -1.2022e-01, -9.9059e-02,  7.7185e-01,\n",
       "            6.7144e-01,  5.2815e-01, -6.9462e-01, -8.3402e-01,  5.2925e-01,\n",
       "            6.6852e-01, -3.0353e-01, -4.7525e-01, -5.5234e-01,  2.0982e-01,\n",
       "            2.1835e-01,  2.1234e-01, -4.6668e-01,  5.5422e-01, -1.4667e-01,\n",
       "            1.0000e+00,  1.7381e-01, -3.6607e-01, -9.2142e-01,  1.3866e-01,\n",
       "           -1.2236e-01,  1.0000e+00, -7.4575e-01, -9.3337e-01,  2.1803e-01,\n",
       "           -6.1165e-01, -5.8932e-01,  3.2490e-01, -5.1111e-02, -6.8875e-01,\n",
       "           -8.9673e-01,  8.0094e-01,  7.1993e-01, -5.5071e-01,  4.8104e-01,\n",
       "           -1.6852e-01, -4.9089e-01,  4.6908e-02,  8.5191e-01,  9.7482e-01,\n",
       "            6.8133e-01,  7.8281e-01, -4.7172e-01, -1.7891e-01,  9.4061e-01,\n",
       "            2.1040e-01,  2.4149e-01,  2.8878e-02,  1.0000e+00,  2.5238e-01,\n",
       "           -8.8731e-01,  3.1861e-01, -9.5687e-01, -1.6375e-02, -9.1358e-01,\n",
       "            3.1224e-01,  8.8811e-04,  8.7901e-01, -2.4456e-01,  9.0650e-01,\n",
       "           -7.6858e-01, -1.5847e-02, -6.1419e-01, -3.1472e-01,  3.1566e-01,\n",
       "           -9.0968e-01, -9.6916e-01, -9.8129e-01,  6.2493e-01, -3.0652e-01,\n",
       "            1.3722e-02,  2.5826e-01,  1.1356e-01,  4.8071e-01,  3.3928e-01,\n",
       "           -1.0000e+00,  9.2649e-01,  3.0368e-01,  6.6872e-01,  9.4624e-01,\n",
       "            7.1238e-01,  5.5077e-01,  2.9763e-01, -9.7580e-01, -9.3272e-01,\n",
       "           -1.9054e-01, -3.0725e-01,  6.5145e-01,  6.7116e-01,  7.8639e-01,\n",
       "            4.9389e-01, -4.8794e-01, -4.6599e-01, -3.9382e-01, -8.9365e-01,\n",
       "           -9.8810e-01,  3.4561e-01, -5.6238e-01, -8.6042e-01,  9.3059e-01,\n",
       "           -3.7564e-01,  3.3075e-02,  1.0516e-01, -7.6951e-01,  7.6152e-01,\n",
       "            6.6181e-01,  2.7837e-01, -1.4805e-02,  3.8428e-01,  8.6620e-01,\n",
       "            8.3068e-01,  9.7044e-01, -6.6240e-01,  5.3158e-01, -7.0786e-01,\n",
       "            3.4880e-01,  7.1666e-01, -9.0876e-01,  7.2923e-02,  3.2683e-01,\n",
       "           -5.1478e-02,  1.1985e-01, -2.3631e-01, -9.0167e-01,  4.6938e-01,\n",
       "           -1.4087e-01,  4.7705e-01, -1.7930e-01,  1.9209e-01, -3.1615e-01,\n",
       "           -1.7714e-01, -6.7050e-01, -6.1762e-01,  5.6061e-01,  8.9560e-02,\n",
       "            8.6114e-01,  7.2770e-01,  3.1228e-03, -6.3668e-01, -2.7062e-01,\n",
       "           -5.1986e-01, -8.7947e-01,  8.0096e-01,  1.0646e-01,  2.2839e-01,\n",
       "            5.5985e-01, -5.9085e-02,  8.2285e-01, -9.4245e-02, -3.7620e-01,\n",
       "           -2.8923e-01, -5.4916e-01,  7.4290e-01, -7.4558e-01, -4.8259e-01,\n",
       "           -4.8837e-01,  6.1933e-01,  1.6416e-01,  9.9995e-01, -5.2680e-01,\n",
       "           -8.1121e-01, -5.4764e-01, -3.4554e-01,  2.6686e-01, -2.9679e-01,\n",
       "           -1.0000e+00,  4.1533e-01, -6.3294e-01,  3.9021e-01, -7.6766e-01,\n",
       "            7.0091e-01, -6.9273e-01, -9.4550e-01, -6.5196e-02,  4.2914e-01,\n",
       "            7.5215e-01, -3.9960e-01, -7.9125e-01,  4.6079e-01, -7.5502e-01,\n",
       "            9.6701e-01,  7.7197e-01, -6.0142e-01,  5.4734e-01,  5.5510e-01,\n",
       "           -7.1097e-01, -5.5534e-01,  8.1731e-01]]),\n",
       "  (tensor([[[ 0.1686, -0.2858, -0.3261,  ..., -0.0276,  0.0383,  0.1640],\n",
       "            [ 0.5956,  0.5420,  0.0412,  ...,  0.4376,  0.5639,  0.3365],\n",
       "            [ 0.2008, -0.5456,  0.3982,  ..., -0.4162,  0.6189,  0.3440],\n",
       "            ...,\n",
       "            [ 0.1181, -1.2275, -0.4309,  ..., -0.2261, -0.2324, -0.9524],\n",
       "            [-0.1581,  0.4450, -0.1719,  ...,  0.3185,  0.7065,  0.4512],\n",
       "            [-0.1429,  0.0752, -0.0835,  ..., -0.3452,  0.0649, -0.3149]]]),\n",
       "   tensor([[[ 0.0130,  0.0857, -0.2351,  ...,  0.2017, -0.0330,  0.0310],\n",
       "            [ 0.6509,  0.7465,  0.0946,  ...,  0.6639,  0.6038,  0.2406],\n",
       "            [ 0.3709, -0.6704,  0.6867,  ..., -0.5662,  1.4102,  0.4405],\n",
       "            ...,\n",
       "            [ 0.3969, -0.7361, -0.0675,  ..., -0.0391, -0.1221, -0.5502],\n",
       "            [-0.1374,  0.3694, -0.1537,  ...,  0.1582,  0.3786,  0.3368],\n",
       "            [-0.0857,  0.0941, -0.1166,  ..., -0.2362,  0.1862, -0.2186]]]),\n",
       "   tensor([[[-0.1343, -0.1883, -0.4383,  ...,  0.2721,  0.1431, -0.0619],\n",
       "            [ 0.4882,  0.7809,  0.2939,  ...,  0.6842,  0.3392,  0.4651],\n",
       "            [ 0.5090, -0.2702,  0.7079,  ..., -1.1010,  1.8665,  0.5992],\n",
       "            ...,\n",
       "            [ 0.4307, -0.8167, -0.0975,  ..., -0.0439,  0.0327, -0.6908],\n",
       "            [-0.0615,  0.1029,  0.2736,  ...,  0.0785,  0.2081,  0.4342],\n",
       "            [-0.1368, -0.0648,  0.0444,  ..., -0.0832,  0.2256, -0.3708]]]),\n",
       "   tensor([[[-0.0087, -0.3578, -0.1933,  ...,  0.2554,  0.2354,  0.1450],\n",
       "            [ 0.5317,  0.5783,  0.2418,  ...,  0.5671,  0.4576,  0.4805],\n",
       "            [ 0.7077, -0.3524,  1.1043,  ..., -0.6134,  1.4829, -0.1043],\n",
       "            ...,\n",
       "            [ 0.5239, -1.0853,  0.0658,  ...,  0.1012,  0.3250, -0.7111],\n",
       "            [-0.0092, -0.0041,  0.3286,  ..., -0.0544,  0.0197,  0.1089],\n",
       "            [-0.0528, -0.0852,  0.1212,  ...,  0.0275,  0.0377, -0.0468]]]),\n",
       "   tensor([[[ 0.0173, -0.6972, -0.3251,  ...,  0.3371,  0.3401,  0.4251],\n",
       "            [ 0.4907,  0.3050,  0.6145,  ...,  0.1432,  0.4554,  0.4227],\n",
       "            [ 0.9022, -0.8332,  1.0876,  ..., -0.2428,  1.3009,  0.0198],\n",
       "            ...,\n",
       "            [ 0.4669, -0.6190, -0.1196,  ...,  0.4527,  0.5768, -0.3933],\n",
       "            [ 0.0172, -0.2359,  0.1483,  ..., -0.1005,  0.1393,  0.2204],\n",
       "            [-0.0192, -0.0530,  0.0210,  ...,  0.0115,  0.0458, -0.0377]]]),\n",
       "   tensor([[[ 0.0386, -0.7583, -0.1136,  ...,  0.0775,  0.1952,  0.4529],\n",
       "            [ 0.6906,  0.4647,  0.4957,  ...,  0.0828,  0.3029,  0.4206],\n",
       "            [ 0.8344, -0.8235,  0.3438,  ..., -0.7253,  0.9412, -0.1774],\n",
       "            ...,\n",
       "            [ 0.3582, -0.5931,  0.1782,  ...,  0.6122,  0.6450, -0.3202],\n",
       "            [ 0.2684, -0.0760,  0.2212,  ..., -0.0869,  0.1989,  0.3655],\n",
       "            [-0.0155, -0.0310,  0.0338,  ...,  0.0154,  0.0056, -0.0273]]]),\n",
       "   tensor([[[ 0.2224, -1.2075, -0.1061,  ..., -0.3009,  0.4584,  0.4567],\n",
       "            [ 0.3992, -0.0306,  0.4000,  ..., -0.1411,  0.4625, -0.0151],\n",
       "            [ 0.6649, -0.9698,  0.2214,  ..., -0.5449,  1.1090, -0.2379],\n",
       "            ...,\n",
       "            [ 0.2555, -0.4454,  0.3684,  ...,  0.1874,  0.4936, -0.5249],\n",
       "            [-0.0144, -0.1956, -0.0635,  ..., -0.4996,  0.1376,  0.4432],\n",
       "            [ 0.0201, -0.0408, -0.0063,  ..., -0.0174, -0.0143, -0.0477]]]),\n",
       "   tensor([[[ 0.3008, -1.1506, -0.1726,  ..., -0.6031,  0.7266,  0.5517],\n",
       "            [ 0.3343,  0.0598,  0.3834,  ..., -0.0754,  0.5456,  0.2857],\n",
       "            [ 0.3123, -1.0409,  0.1790,  ..., -1.0639,  0.7737, -0.4183],\n",
       "            ...,\n",
       "            [ 0.5477, -0.4033,  0.3581,  ...,  0.0348,  0.6326, -0.4790],\n",
       "            [ 0.0938, -0.7556, -0.6362,  ..., -0.8945,  0.3228,  0.4941],\n",
       "            [ 0.0184, -0.0296, -0.0111,  ..., -0.0287,  0.0250, -0.0486]]]),\n",
       "   tensor([[[ 0.1653, -1.0950, -0.4525,  ..., -0.7500,  0.7579,  0.7313],\n",
       "            [-0.0138,  0.0731,  0.6128,  ..., -0.1126,  0.6375,  0.6633],\n",
       "            [-0.0191, -1.2931,  0.7316,  ..., -1.0184,  1.0605, -0.4975],\n",
       "            ...,\n",
       "            [ 0.2691,  0.0886,  0.2394,  ..., -0.0968,  0.9363, -0.5861],\n",
       "            [-0.4416, -0.8237, -0.8789,  ..., -0.8322,  0.4349,  0.4773],\n",
       "            [ 0.0318, -0.0313,  0.0471,  ..., -0.0293, -0.0299, -0.0713]]]),\n",
       "   tensor([[[ 0.0485, -1.3996, -0.4254,  ..., -0.3020,  0.8508,  0.2701],\n",
       "            [-0.1065, -0.4571,  0.5319,  ..., -0.1835,  0.9651,  0.2593],\n",
       "            [-0.1992, -1.6536,  1.2141,  ..., -0.6131,  1.0226, -0.2953],\n",
       "            ...,\n",
       "            [ 0.1657, -0.0478,  0.2656,  ...,  0.2259,  0.9136, -0.7362],\n",
       "            [-0.8857, -1.3178, -0.6104,  ..., -0.3032,  0.6336, -0.3336],\n",
       "            [ 0.0488, -0.0094,  0.0160,  ..., -0.0772, -0.0435, -0.0454]]]),\n",
       "   tensor([[[-0.3506, -1.3064, -0.0372,  ..., -0.2754,  0.4478, -0.0627],\n",
       "            [-0.1556, -0.4793,  0.3534,  ..., -0.1902,  1.2394,  0.0680],\n",
       "            [-0.1710, -1.7218,  1.2661,  ..., -0.3929,  0.8562, -0.4936],\n",
       "            ...,\n",
       "            [-0.1449, -0.2386,  0.3346,  ..., -0.0302,  0.7176, -0.9448],\n",
       "            [-0.8297, -1.1609, -0.4085,  ...,  0.0360,  0.5171, -0.5234],\n",
       "            [ 0.0287,  0.0020, -0.0535,  ...,  0.0813, -0.0212, -0.0032]]]),\n",
       "   tensor([[[-0.0978, -0.9995,  0.0536,  ..., -0.3511,  0.4301,  0.0221],\n",
       "            [ 0.0993, -0.4581,  0.2671,  ..., -0.5003,  1.0959,  0.4620],\n",
       "            [ 0.1799, -1.4585,  1.1729,  ..., -0.2949,  0.8193, -0.1403],\n",
       "            ...,\n",
       "            [-0.1456,  0.0750,  0.5953,  ..., -0.4233,  0.7256, -0.4855],\n",
       "            [-0.4965, -1.5470, -0.1087,  ...,  0.4229,  0.7194, -0.7450],\n",
       "            [ 0.0305,  0.0332, -0.0291,  ...,  0.0165, -0.0371,  0.0216]]]),\n",
       "   tensor([[[-0.1197, -0.5103,  0.1755,  ..., -0.4528,  0.5111,  0.3569],\n",
       "            [-0.0253, -0.4410,  0.0922,  ..., -0.4358,  0.9313,  0.2552],\n",
       "            [ 0.1569, -0.8916,  0.9990,  ..., -0.0488,  0.7035, -0.1455],\n",
       "            ...,\n",
       "            [ 0.0419,  0.0642,  0.2044,  ..., -0.0439,  0.3478, -0.2916],\n",
       "            [-0.0450, -1.2345, -0.0363,  ...,  0.6918,  0.7434, -0.7393],\n",
       "            [ 0.8274,  0.0998, -0.2487,  ..., -0.0129, -0.5028, -0.2075]]]))),\n",
       " tensor([[  101,  1037, 20228,  7716,  4667,  9458, 12661,  2008,  1005,  1055,\n",
       "           2061,  6228,  2017,  2064,  5437,  1996, 21956,  2006,  1996,  5436,\n",
       "          21438,  1012,   102]]),\n",
       " \"a plodding teen remake that's so mechanical you can smell the grease on the plot twists .\")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vectors[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train classifier using each level of representation in BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7673545966228893\n",
      "1 0.7725140712945591\n",
      "2 0.7678236397748592\n",
      "3 0.7612570356472795\n",
      "4 0.7696998123827392\n",
      "5 0.7781425891181989\n",
      "6 0.7790806754221389\n",
      "7 0.801125703564728\n",
      "8 0.8147279549718575\n",
      "9 0.8198874296435272\n",
      "10 0.8180112570356473\n",
      "11 0.8189493433395872\n",
      "12 0.8039399624765479\n"
     ]
    }
   ],
   "source": [
    "for layer in range(len(output[-1])):\n",
    "    train_vectors_ = []\n",
    "    for vecs, input_ids, str_ in train_vectors:\n",
    "        long_vecs = vecs[-1][layer].detach().numpy().sum(axis=1).flatten()\n",
    "        train_vectors_.append(long_vecs)\n",
    "    model2 = LogisticRegressionCV(max_iter=2000)\n",
    "    model2.fit(X=train_vectors_, y=train_reviews['type'])\n",
    "    test_vectors_ = []\n",
    "    for vecs, input_ids, str_ in test_vectors:\n",
    "        long_vecs = vecs[-1][layer].detach().numpy().sum(axis=1).flatten()\n",
    "        test_vectors_.append(long_vecs)\n",
    "    print(layer, model2.score(X=test_vectors_, y=test_reviews['type']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
